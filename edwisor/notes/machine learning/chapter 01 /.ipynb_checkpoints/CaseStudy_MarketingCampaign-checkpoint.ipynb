{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fancyimpute import KNN   \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "from random import randrange, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maneeshagvs/Documents/datasets'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/maneeshagvs/documents/datasets')\n",
    "\n",
    "os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "marketing_train = pd.read_csv(\"marketing_tr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis\n",
    "marketing_train['schooling'] = marketing_train['schooling'].replace(\"illiterate\", \"unknown\")\n",
    "marketing_train['schooling'] = marketing_train['schooling'].replace([\"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"professional.course\"], \"high.school\")\n",
    "marketing_train['default'] = marketing_train['default'].replace(\"yes\", \"unknown\")\n",
    "marketing_train['marital'] = marketing_train['marital'].replace(\"unknown\", \"married\")\n",
    "marketing_train['month'] = marketing_train['month'].replace([\"sep\",\"oct\",\"mar\",\"dec\"], \"dec\")\n",
    "marketing_train['month'] = marketing_train['month'].replace([\"aug\",\"jul\",\"jun\",\"may\",\"nov\"], \"jun\")\n",
    "marketing_train['loan'] = marketing_train['loan'].replace(\"unknown\", \"no\")\n",
    "marketing_train['profession'] = marketing_train['profession'].replace([\"management\",\"unknown\",\"unemployed\",\"admin.\"], \"admin.\")\n",
    "marketing_train['profession'] = marketing_train['profession'].replace([\"blue-collar\",\"housemaid\",\"services\",\"self-employed\",\"entrepreneur\",\"technician\"], \"blue-collar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with missing percentage\n",
    "missing_val = pd.DataFrame(marketing_train.isnull().sum())\n",
    "\n",
    "#Reset index\n",
    "missing_val = missing_val.reset_index()\n",
    "\n",
    "#Rename variable\n",
    "missing_val = missing_val.rename(columns = {'index': 'Variables', 0: 'Missing_percentage'})\n",
    "\n",
    "#Calculate percentage\n",
    "missing_val['Missing_percentage'] = (missing_val['Missing_percentage']/len(marketing_train))*100\n",
    "\n",
    "#descending order\n",
    "missing_val = missing_val.sort_values('Missing_percentage', ascending = False).reset_index(drop = True)\n",
    "\n",
    "#save output results \n",
    "missing_val.to_csv(\"Miising_perc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imputation method\n",
    "#Actual value = 29\n",
    "#Mean = 40.01\n",
    "#Median = 38\n",
    "#KNN = 29.35\n",
    "\n",
    "#create missing value\n",
    "#marketing_train['custAge'].loc[70] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute with mean\n",
    "#marketing_train['custAge'] = marketing_train['custAge'].fillna(marketing_train['custAge'].mean())\n",
    "\n",
    "#Impute with median\n",
    "#marketing_train['custAge'] = marketing_train['custAge'].fillna(marketing_train['custAge'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN imputation\n",
    "#Assigning levels to the categories\n",
    "lis = []\n",
    "for i in range(0, marketing_train.shape[1]):\n",
    "    #print(i)\n",
    "    if(marketing_train.iloc[:,i].dtypes == 'object'):\n",
    "        marketing_train.iloc[:,i] = pd.Categorical(marketing_train.iloc[:,i])\n",
    "        #print(marketing_train[[i]])\n",
    "        marketing_train.iloc[:,i] = marketing_train.iloc[:,i].cat.codes \n",
    "        marketing_train.iloc[:,i] = marketing_train.iloc[:,i].astype('object')\n",
    "        \n",
    "        lis.append(marketing_train.columns[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace -1 with NA to impute\n",
    "for i in range(0, marketing_train.shape[1]):\n",
    "    marketing_train.iloc[:,i] = marketing_train.iloc[:,i].replace(-1, np.nan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DataScience/lib/python2.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/7414 with 0 missing, elapsed time: 13.046\n",
      "Imputing row 101/7414 with 0 missing, elapsed time: 13.047\n",
      "Imputing row 201/7414 with 0 missing, elapsed time: 13.049\n",
      "Imputing row 301/7414 with 0 missing, elapsed time: 13.049\n",
      "Imputing row 401/7414 with 0 missing, elapsed time: 13.050\n",
      "Imputing row 501/7414 with 0 missing, elapsed time: 13.051\n",
      "Imputing row 601/7414 with 0 missing, elapsed time: 13.052\n",
      "Imputing row 701/7414 with 0 missing, elapsed time: 13.054\n",
      "Imputing row 801/7414 with 0 missing, elapsed time: 13.055\n",
      "Imputing row 901/7414 with 0 missing, elapsed time: 13.056\n",
      "Imputing row 1001/7414 with 0 missing, elapsed time: 13.059\n",
      "Imputing row 1101/7414 with 0 missing, elapsed time: 13.060\n",
      "Imputing row 1201/7414 with 0 missing, elapsed time: 13.060\n",
      "Imputing row 1301/7414 with 0 missing, elapsed time: 13.062\n",
      "Imputing row 1401/7414 with 0 missing, elapsed time: 13.063\n",
      "Imputing row 1501/7414 with 0 missing, elapsed time: 13.064\n",
      "Imputing row 1601/7414 with 0 missing, elapsed time: 13.065\n",
      "Imputing row 1701/7414 with 0 missing, elapsed time: 13.066\n",
      "Imputing row 1801/7414 with 0 missing, elapsed time: 13.067\n",
      "Imputing row 1901/7414 with 0 missing, elapsed time: 13.068\n",
      "Imputing row 2001/7414 with 0 missing, elapsed time: 13.069\n",
      "Imputing row 2101/7414 with 0 missing, elapsed time: 13.070\n",
      "Imputing row 2201/7414 with 0 missing, elapsed time: 13.071\n",
      "Imputing row 2301/7414 with 0 missing, elapsed time: 13.071\n",
      "Imputing row 2401/7414 with 0 missing, elapsed time: 13.072\n",
      "Imputing row 2501/7414 with 0 missing, elapsed time: 13.072\n",
      "Imputing row 2601/7414 with 0 missing, elapsed time: 13.073\n",
      "Imputing row 2701/7414 with 0 missing, elapsed time: 13.073\n",
      "Imputing row 2801/7414 with 0 missing, elapsed time: 13.074\n",
      "Imputing row 2901/7414 with 0 missing, elapsed time: 13.074\n",
      "Imputing row 3001/7414 with 0 missing, elapsed time: 13.075\n",
      "Imputing row 3101/7414 with 0 missing, elapsed time: 13.076\n",
      "Imputing row 3201/7414 with 0 missing, elapsed time: 13.077\n",
      "Imputing row 3301/7414 with 0 missing, elapsed time: 13.078\n",
      "Imputing row 3401/7414 with 0 missing, elapsed time: 13.080\n",
      "Imputing row 3501/7414 with 0 missing, elapsed time: 13.080\n",
      "Imputing row 3601/7414 with 0 missing, elapsed time: 13.081\n",
      "Imputing row 3701/7414 with 0 missing, elapsed time: 13.083\n",
      "Imputing row 3801/7414 with 0 missing, elapsed time: 13.084\n",
      "Imputing row 3901/7414 with 0 missing, elapsed time: 13.085\n",
      "Imputing row 4001/7414 with 0 missing, elapsed time: 13.085\n",
      "Imputing row 4101/7414 with 0 missing, elapsed time: 13.086\n",
      "Imputing row 4201/7414 with 0 missing, elapsed time: 13.086\n",
      "Imputing row 4301/7414 with 0 missing, elapsed time: 13.087\n",
      "Imputing row 4401/7414 with 0 missing, elapsed time: 13.088\n",
      "Imputing row 4501/7414 with 0 missing, elapsed time: 13.089\n",
      "Imputing row 4601/7414 with 0 missing, elapsed time: 13.091\n",
      "Imputing row 4701/7414 with 0 missing, elapsed time: 13.091\n",
      "Imputing row 4801/7414 with 0 missing, elapsed time: 13.092\n",
      "Imputing row 4901/7414 with 0 missing, elapsed time: 13.092\n",
      "Imputing row 5001/7414 with 0 missing, elapsed time: 13.093\n",
      "Imputing row 5101/7414 with 0 missing, elapsed time: 13.093\n",
      "Imputing row 5201/7414 with 0 missing, elapsed time: 13.094\n",
      "Imputing row 5301/7414 with 0 missing, elapsed time: 13.094\n",
      "Imputing row 5401/7414 with 0 missing, elapsed time: 13.095\n",
      "Imputing row 5501/7414 with 0 missing, elapsed time: 13.096\n",
      "Imputing row 5601/7414 with 0 missing, elapsed time: 13.096\n",
      "Imputing row 5701/7414 with 0 missing, elapsed time: 13.097\n",
      "Imputing row 5801/7414 with 0 missing, elapsed time: 13.098\n",
      "Imputing row 5901/7414 with 0 missing, elapsed time: 13.100\n",
      "Imputing row 6001/7414 with 0 missing, elapsed time: 13.101\n",
      "Imputing row 6101/7414 with 0 missing, elapsed time: 13.101\n",
      "Imputing row 6201/7414 with 0 missing, elapsed time: 13.102\n",
      "Imputing row 6301/7414 with 0 missing, elapsed time: 13.103\n",
      "Imputing row 6401/7414 with 0 missing, elapsed time: 13.104\n",
      "Imputing row 6501/7414 with 0 missing, elapsed time: 13.104\n",
      "Imputing row 6601/7414 with 0 missing, elapsed time: 13.105\n",
      "Imputing row 6701/7414 with 0 missing, elapsed time: 13.105\n",
      "Imputing row 6801/7414 with 0 missing, elapsed time: 13.106\n",
      "Imputing row 6901/7414 with 0 missing, elapsed time: 13.107\n",
      "Imputing row 7001/7414 with 0 missing, elapsed time: 13.108\n",
      "Imputing row 7101/7414 with 0 missing, elapsed time: 13.108\n",
      "Imputing row 7201/7414 with 0 missing, elapsed time: 13.109\n",
      "Imputing row 7301/7414 with 0 missing, elapsed time: 13.110\n",
      "Imputing row 7401/7414 with 0 missing, elapsed time: 13.110\n"
     ]
    }
   ],
   "source": [
    "#KNN Complete is replaced by .fit_transform to match the SK_learn \n",
    "\n",
    "#Apply KNN imputation algorithm\n",
    "marketing_train = pd.DataFrame(KNN(k = 3).fit_transform(marketing_train), columns = marketing_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into proper datatypes\n",
    "\n",
    "#solution for round off error - https://stackoverflow.com/questions/19387608/attributeerror-rint-when-using-numpy-round\n",
    "\n",
    "\n",
    "for i in lis:\n",
    "    marketing_train.loc[:,i] = np.around(marketing_train.loc[:,i].astype(np.double),3)\n",
    "    marketing_train.loc[:,i] = marketing_train.loc[:,i].astype('object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = marketing_train.copy()\n",
    "#marketing_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Plot boxplot to visualize Outliers\n",
    "# %matplotlib inline  \n",
    "# plt.boxplot(marketing_train['custAge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save numeric names\n",
    "cnames =  [\"custAge\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\",\n",
    "           \"nr.employed\", \"pmonths\", \"pastEmail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Detect and delete outliers from data\n",
    "# for i in cnames:\n",
    "#     print(i)\n",
    "#     q75, q25 = np.percentile(marketing_train.loc[:,i], [75 ,25])\n",
    "#     iqr = q75 - q25\n",
    "\n",
    "#     min = q25 - (iqr*1.5)\n",
    "#     max = q75 + (iqr*1.5)\n",
    "#     print(min)\n",
    "#     print(max)\n",
    "    \n",
    "#     marketing_train = marketing_train.drop(marketing_train[marketing_train.loc[:,i] < min].index)\n",
    "#     marketing_train = marketing_train.drop(marketing_train[marketing_train.loc[:,i] > max].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect and replace with NA\n",
    "# #Extract quartiles\n",
    "# q75, q25 = np.percentile(marketing_train['custAge'], [75 ,25])\n",
    "\n",
    "# #Calculate IQR\n",
    "# iqr = q75 - q25\n",
    "\n",
    "# #Calculate inner and outer fence\n",
    "# minimum = q25 - (iqr*1.5)\n",
    "# maximum = q75 + (iqr*1.5)\n",
    "\n",
    "# #Replace with NA\n",
    "# marketing_train.loc[marketing_train['custAge'] < minimum,:'custAge'] = np.nan\n",
    "# marketing_train.loc[marketing_train['custAge'] > maximum,:'custAge'] = np.nan\n",
    "\n",
    "# #Calculate missing value\n",
    "# missing_val = pd.DataFrame(marketing_train.isnull().sum())\n",
    "\n",
    "# #Impute with KNN\n",
    "# marketing_train = pd.DataFrame(KNN(k = 3).complete(marketing_train), columns = marketing_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correlation analysis\n",
    "#Correlation plot\n",
    "df_corr = marketing_train.loc[:,cnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the width and hieght of the plot\n",
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "#Generate correlation matrix\n",
    "corr = df_corr.corr()\n",
    "\n",
    "#Plot using seaborn library\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Chisquare test of independence\n",
    "#Save categorical variables\n",
    "cat_names = [\"profession\", \"marital\", \"schooling\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day_of_week\", \"poutcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loop for chi square values\n",
    "for i in cat_names:\n",
    "    print(i)\n",
    "    chi2, p, dof, ex = chi2_contingency(pd.crosstab(marketing_train['responded'], marketing_train[i]))\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_train = marketing_train.drop(['pdays', 'emp.var.rate', 'day_of_week', 'loan', 'housing'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = marketing_train.copy()\n",
    "#marketing_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normality check\n",
    "%matplotlib inline  \n",
    "plt.hist(marketing_train['campaign'], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnames = [\"custAge\",\"campaign\",\"previous\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\",\n",
    "           \"pmonths\",\"pastEmail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nomalisation\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    marketing_train[i] = (marketing_train[i] - min(marketing_train[i]))/(max(marketing_train[i]) - min(marketing_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Standarisation\n",
    "# for i in cnames:\n",
    "#     print(i)\n",
    "#     marketing_train[i] = (marketing_train[i] - marketing_train[i].mean())/marketing_train[i].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Simple random sampling\n",
    "#Sim_Sampling = marketing_train.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##Systematic Sampling\n",
    "# #Calculate the K value\n",
    "# k = len(marketing_train)/3500\n",
    "\n",
    "# # Generate a random number using simple random sampling\n",
    "# RandNum = randrange(0, 5)\n",
    "\n",
    "# #select Kth observation starting from RandNum\n",
    "# Sys_Sampling = marketing_train.iloc[RandNum::k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Stratified sampling\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# #Select categorical variable\n",
    "# y = marketing_train['profession']\n",
    "\n",
    "#select subset using stratified Sampling\n",
    "#Rest, Sample = train_test_split(marketing_train, test_size = 0.6, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marketing_train = pd.read_csv(\"marketing_train_Model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries for decision tree\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace target categories with Yes or No\n",
    "marketing_train['responded'] = marketing_train['responded'].replace(0, 'No')\n",
    "marketing_train['responded'] = marketing_train['responded'].replace(1, 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into train and test\n",
    "X = marketing_train.values[:, 0:16]\n",
    "Y = marketing_train.values[:,16]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "C50_model = tree.DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train)\n",
    "\n",
    "#predict new test cases\n",
    "C50_Predictions = C50_model.predict(X_test)\n",
    "\n",
    "#Create dot file to visualise tree  #http://webgraphviz.com/\n",
    "# dotfile = open(\"pt.dot\", 'w')\n",
    "# df = tree.export_graphviz(C50_model, out_file=dotfile, feature_names = marketing_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix \n",
    "# CM = confusion_matrix(y_test, y_pred)\n",
    "CM = pd.crosstab(y_test, C50_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "#(FN*100)/(FN+TP)\n",
    "\n",
    "#Results\n",
    "#Accuracy: 84.49\n",
    "#FNR: 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators = 20).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_Predictions = RF_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix \n",
    "# CM = confusion_matrix(y_test, y_pred)\n",
    "CM = pd.crosstab(y_test, RF_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "#(FN*100)/(FN+TP)\n",
    "\n",
    "#Accuracy: 88\n",
    "#FNR: 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let us prepare data for logistic regression\n",
    "#replace target categories with Yes or No\n",
    "marketing_train['responded'] = marketing_train['responded'].replace('No', 0)\n",
    "marketing_train['responded'] = marketing_train['responded'].replace('Yes', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create logistic data. Save target variable first\n",
    "marketing_train_logit = pd.DataFrame(marketing_train['responded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add continous variables\n",
    "marketing_train_logit = marketing_train_logit.join(marketing_train[cnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_train_logit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create dummies for categorical variables\n",
    "cat_names = [\"profession\", \"marital\", \"schooling\", \"default\", \"contact\", \"month\", \"poutcome\"]\n",
    "\n",
    "for i in cat_names:\n",
    "    temp = pd.get_dummies(marketing_train[i], prefix = i)\n",
    "    marketing_train_logit = marketing_train_logit.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sample_Index = np.random.rand(len(marketing_train_logit)) < 0.8\n",
    "\n",
    "train = marketing_train_logit[Sample_Index]\n",
    "test = marketing_train_logit[~Sample_Index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select column indexes for independent variables\n",
    "train_cols = train.columns[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built Logistic Regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "logit = sm.Logit(train['responded'], train[train_cols]).fit()\n",
    "\n",
    "logit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict test data\n",
    "test['Actual_prob'] = logit.predict(test[train_cols])\n",
    "\n",
    "test['ActualVal'] = 1\n",
    "test.loc[test.Actual_prob < 0.5, 'ActualVal'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build confusion matrix\n",
    "CM = pd.crosstab(test['responded'], test['ActualVal'])\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "(FN*100)/(FN+TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy: 90\n",
    "#FNR: 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_model = KNeighborsClassifier(n_neighbors = 9).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict test cases\n",
    "KNN_Predictions = KNN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build confusion matrix\n",
    "CM = pd.crosstab(y_test, KNN_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "(FN*100)/(FN+TP)\n",
    "\n",
    "#Accuracy: 89\n",
    "#FNR: 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Naive Bayes implementation\n",
    "NB_model = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict test cases\n",
    "NB_Predictions = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build confusion matrix\n",
    "CM = pd.crosstab(y_test, NB_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "#((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "(FN*100)/(FN+TP)\n",
    "\n",
    "#Accuracy: 81\n",
    "#FNR: 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load required libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Estimate optimum number of clusters\n",
    "cluster_range = range( 1, 20 )\n",
    "cluster_errors = []\n",
    "\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans(num_clusters).fit(df.iloc[:,0:4])\n",
    "    cluster_errors.append(clusters.inertia_)\n",
    "    \n",
    "#Create dataframe with cluster errors\n",
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot line chart to visualise number of clusters\n",
    "%matplotlib inline  \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implement kmeans\n",
    "kmeans_model = KMeans(n_clusters = 3).fit(df.iloc[:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize output\n",
    "pd.crosstab(df['Species'], kmeans_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_range = range( 1, 20 )\n",
    "cluster_errors = []\n",
    "\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans( num_clusters )\n",
    "    clusters.fit( df.iloc[:,0:4] )\n",
    "    cluster_errors.append( clusters.inertia_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
